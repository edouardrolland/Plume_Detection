{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf96fb4",
   "metadata": {
    "id": "7cf96fb4"
   },
   "source": [
    "# Post-Training Quantization in PyTorch using the Model Compression Toolkit (MCT)\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_post_training_quantization.ipynb)\n",
    "\n",
    "## Overview\n",
    "This quick-start guide explains how to use the **Model Compression Toolkit (MCT)** to quantize a PyTorch model. We will load a pre-trained model and  quantize it using the MCT with **Post-Training Quatntization (PTQ)**. Finally, we will evaluate the quantized model and export it to an ONNX file.\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. Loading and preprocessing ImageNet’s validation dataset.\n",
    "2. Constructing an unlabeled representative dataset.\n",
    "3. Post-Training Quantization using MCT.\n",
    "4. Accuracy evaluation of the floating-point and the quantized models.\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e0bb04",
   "metadata": {
    "id": "89e0bb04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5441efd2978cea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82928d0",
   "metadata": {
    "id": "a82928d0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.datasets import ImageNet\n",
    "import numpy as np\n",
    "import random\n",
    "from ultralytics  import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2556ce8144e1d3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load a pre-trained MobileNetV2 model from torchvision, in 32-bits floating-point precision format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a302610146f1ec3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "float_model = YOLO('plume.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df074784266e12e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset preparation\n",
    "### Download ImageNet validation set\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8a3327f28c20caf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff2ea33659f0c1a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Extract ImageNet validation dataset using torchvision \"datasets\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "18f57edc3b87cad3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/your/dataset.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Load your dataset as a pandas DataFrame (this could be from a CSV, Parquet file, etc.)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/path/to/your/dataset.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Example: replace with your actual file path\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Now create the VolcanicPlumeDataset using the dataframe\u001b[39;00m\n\u001b[1;32m     61\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m VolcanicPlumeDataset(dataframe, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/your/dataset.parquet'"
     ]
    }
   ],
   "source": [
    "splits = {\n",
    "    'train': 'hf://datasets/edouard-rolland/volcanic-plumes/data/train-00000-of-00001.parquet',\n",
    "    'validation': 'hf://datasets/edouard-rolland/volcanic-plumes/data/validation-00000-of-00001.parquet',\n",
    "    'test': 'hf://datasets/edouard-rolland/volcanic-plumes/data/test-00000-of-00001.parquet'\n",
    "}\n",
    "\n",
    "# Load each split into a DataFrame\n",
    "train_df = pd.read_parquet(splits['train'])\n",
    "validation_df = pd.read_parquet(splits['validation'])\n",
    "test_df = pd.read_parquet(splits['test'])\n",
    "\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Define the transformation to convert PIL images to tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size (optional)\n",
    "    transforms.ToTensor()  # Convert the PIL Image to a tensor\n",
    "])\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "from torchvision import transforms\n",
    "\n",
    "class VolcanicPlumeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        # Ensure that the dataframe is a pandas DataFrame\n",
    "        if not isinstance(dataframe, pd.DataFrame):\n",
    "            raise TypeError(f\"Expected dataframe to be a pandas DataFrame, got {type(dataframe)}\")\n",
    "        \n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(type(self.dataframe))  # Check if it's a pandas DataFrame\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_bytes = row['image']['bytes']\n",
    "        image = Image.open(io.BytesIO(image_bytes))  # Convert bytes to PIL Image\n",
    "\n",
    "        # Apply transformation\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = row['objects']\n",
    "        return image, label\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load your dataset as a pandas DataFrame (this could be from a CSV, Parquet file, etc.)\n",
    "dataframe = pd.read_parquet('/path/to/your/dataset.parquet')  # Example: replace with your actual file path\n",
    "\n",
    "# Now create the VolcanicPlumeDataset using the dataframe\n",
    "train_dataset = VolcanicPlumeDataset(dataframe, transform=transform)\n",
    "\n",
    "# Create a DataLoader to load the data in batches\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_dataset = VolcanicPlumeDataset(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0321aad",
   "metadata": {
    "id": "c0321aad"
   },
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "618975be",
   "metadata": {
    "id": "618975be"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected dataframe to be a pandas DataFrame, got <class '__main__.VolcanicPlumeDataset'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mVolcanicPlumeDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Now, you can create the DataLoader\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "Cell \u001b[0;32mIn[92], line 33\u001b[0m, in \u001b[0;36mVolcanicPlumeDataset.__init__\u001b[0;34m(self, dataframe, transform)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataframe, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Ensure that the dataframe is a pandas DataFrame\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataframe, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m---> 33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dataframe to be a pandas DataFrame, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dataframe)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe \u001b[38;5;241m=\u001b[39m dataframe\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected dataframe to be a pandas DataFrame, got <class '__main__.VolcanicPlumeDataset'>"
     ]
    }
   ],
   "source": [
    "train_dataset = VolcanicPlumeDataset(train_dataset, transform=transform)\n",
    "\n",
    "\n",
    "# Now, you can create the DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a transformation to convert the image to a tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    for _ in range(n_iter):\n",
    "        image, label = next(dataloader_iter)  # The image is already a tensor\n",
    "        yield [image]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33271e23c3eff3b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Target Platform Capabilities (TPC)\n",
    "In addition, MCT optimizes the model for dedicated hardware platforms. This is done using TPC (for more details, please visit our [documentation](https://sony.github.io/model_optimization/docs/api/api_docs/modules/target_platform.html)). Here, we use the default Pytorch TPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae04779a863facd7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "# Get a TargetPlatformCapabilities object that models the hardware platform for the quantized model inference. Here, for example, we use the default platform that is attached to a Pytorch layers representation.\n",
    "target_platform_cap = mct.get_target_platform_capabilities('pytorch', 'default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a92bee",
   "metadata": {
    "id": "d0a92bee"
   },
   "source": [
    "## Post-Training Quantization using MCT\n",
    "Now for the exciting part! Let’s run PTQ on the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "63f695dd",
   "metadata": {
    "id": "63f695dd"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VolcanicPlumeDataset' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m quantized_model, quantization_info \u001b[38;5;241m=\u001b[39m \u001b[43mmct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch_post_training_quantization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfloat_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepresentative_data_gen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepresentative_dataset_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_platform_capabilities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_platform_cap\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/model_compression_toolkit/ptq/pytorch/quantization_facade.py:111\u001b[0m, in \u001b[0;36mpytorch_post_training_quantization\u001b[0;34m(in_module, representative_data_gen, target_resource_utilization, core_config, target_platform_capabilities)\u001b[0m\n\u001b[1;32m    108\u001b[0m fw_impl \u001b[38;5;241m=\u001b[39m PytorchImplementation()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Ignore hessian info service as it is not used here yet.\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m tg, bit_widths_config, _, scheduling_info \u001b[38;5;241m=\u001b[39m \u001b[43mcore_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mrepresentative_data_gen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepresentative_data_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mcore_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mfw_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mfw_impl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mtpc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_platform_capabilities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mtarget_resource_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_resource_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mtb_w\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# At this point, tg is a graph that went through substitutions (such as BN folding) and is\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# ready for quantization (namely, it holds quantization params, etc.) but the weights are\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# not quantized yet. For this reason, we use it to create a graph that acts as a \"float\" graph\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# for things like similarity analyzer (because the quantized and float graph should have the same\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# architecture to find the appropriate compare points for similarity computation).\u001b[39;00m\n\u001b[1;32m    125\u001b[0m similarity_baseline_graph \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tg)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/model_compression_toolkit/core/runner.py:91\u001b[0m, in \u001b[0;36mcore_runner\u001b[0;34m(in_model, representative_data_gen, core_config, fw_info, fw_impl, tpc, target_resource_utilization, running_gptq, tb_w)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mQuantize a trained model using post-training quantization.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03mFirst, the model graph is optimized using several transformations (e.g. folding BatchNormalization to preceding\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Warn is representative dataset has batch-size == 1\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m batch_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrepresentative_data_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     93\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[72], line 15\u001b[0m, in \u001b[0;36mrepresentative_dataset_gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(dataloader)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[0;32m---> 15\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# The image is already a tensor\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m [image]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[67], line 32\u001b[0m, in \u001b[0;36mVolcanicPlumeDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 32\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[idx]\n\u001b[1;32m     33\u001b[0m     image_bytes \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     34\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(image_bytes))  \u001b[38;5;66;03m# Convert bytes to PIL Image\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VolcanicPlumeDataset' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "        in_module=float_model,\n",
    "        representative_data_gen=representative_dataset_gen,\n",
    "        target_platform_capabilities=target_platform_cap\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3521637",
   "metadata": {
    "id": "d3521637"
   },
   "source": [
    "Our model is now quantized. MCT has created a simulated quantized model within the original PyTorch framework by inserting [quantization representation modules](https://github.com/sony/mct_quantizers). These modules, such as `PytorchQuantizationWrapper` and `PytorchActivationQuantizationHolder`, wrap PyTorch layers to simulate the quantization of weights and activations, respectively. While the size of the saved model remains unchanged, all the quantization parameters are stored within these modules and are ready for deployment on the target hardware. In this example, we used the default MCT settings, which compressed the model from 32 bits to 8 bits, resulting in a compression ratio of 4x. Let's print the quantized model and examine the quantization modules:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677bd61c3ab4649",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "In order to evaluate our models, we first need to load the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee11ff6934aa9f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(dataset, batch_size=50, shuffle=False, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ced59d1514509e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we will create a function for evaluating a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f120e924b5d8cf4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate(model, testloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model using a test loader.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # correct += (predicted == labels).sum().item()\n",
    "    val_acc = (100 * correct / total)\n",
    "    print('Accuracy: %.2f%%' % val_acc)\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10499a2b79b19da",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's start with the floating-point model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd038f7aff8cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(float_model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f564f31e253f5c",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the quantized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da2134f0bde415",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(quantized_model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09fa27",
   "metadata": {
    "id": "fd09fa27"
   },
   "source": [
    "You can see that we got a very small degradation with a compression rate of x4 !\n",
    "Now, we can export the quantized model to ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oXMn6bFjbQad",
   "metadata": {
    "id": "oXMn6bFjbQad"
   },
   "outputs": [],
   "source": [
    "mct.exporter.pytorch_export_model(quantized_model, save_model_path='qmodel.onnx', repr_dataset=representative_dataset_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e1572",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a classification model for MNIST in a hardware-friendly manner using MCT. We observed that a 4x compression ratio was achieved with minimal performance degradation.\n",
    "\n",
    "The key advantage of hardware-friendly quantization is that the model can run more efficiently in terms of runtime, power consumption, and memory usage on designated hardware.\n",
    "\n",
    "MCT can deliver competitive results across a wide range of tasks and network architectures. For more details, [check out the paper:](https://arxiv.org/abs/2109.09113).\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
